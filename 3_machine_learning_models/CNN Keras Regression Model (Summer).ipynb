{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and read test and train data set\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>Year</th>\n",
       "      <th>OlympicsSeason</th>\n",
       "      <th>TotalParticipants</th>\n",
       "      <th>GoldMedals</th>\n",
       "      <th>SilverMedals</th>\n",
       "      <th>BronzeMedals</th>\n",
       "      <th>TotalMedals</th>\n",
       "      <th>Win%</th>\n",
       "      <th>GDPPerCapita</th>\n",
       "      <th>...</th>\n",
       "      <th>PrevTotalParticipants</th>\n",
       "      <th>PrevGoldMedals</th>\n",
       "      <th>PrevSilverMedals</th>\n",
       "      <th>PrevBronzeMedals</th>\n",
       "      <th>PrevTotalMedals</th>\n",
       "      <th>PrevWinterTotalParticipants</th>\n",
       "      <th>PrevWinterGoldMedals</th>\n",
       "      <th>PrevWinterSilverMedals</th>\n",
       "      <th>PrevWinterBronzeMedals</th>\n",
       "      <th>PrevWinterTotalMedals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUS</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUT</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>62.50</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEN</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>40.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRA</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>42.31</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBR</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>36.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GER</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>34.04</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRE</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>48</td>\n",
       "      <td>32.43</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HUN</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>33.33</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ITA</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SUI</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>37.50</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CountryCode  Year  OlympicsSeason  TotalParticipants  GoldMedals  \\\n",
       "0         AUS  1896               1                  5           2   \n",
       "1         AUT  1896               1                  8           2   \n",
       "2         DEN  1896               1                 15           1   \n",
       "3         FRA  1896               1                 26           5   \n",
       "4         GBR  1896               1                 25           3   \n",
       "5         GER  1896               1                 94          25   \n",
       "6         GRE  1896               1                148          10   \n",
       "7         HUN  1896               1                 18           2   \n",
       "8         ITA  1896               1                  1           0   \n",
       "9         SUI  1896               1                  8           1   \n",
       "\n",
       "   SilverMedals  BronzeMedals  TotalMedals   Win%  GDPPerCapita  ...  \\\n",
       "0             0             1            3  60.00      6.826319  ...   \n",
       "1             1             2            5  62.50      6.826319  ...   \n",
       "2             2             3            6  40.00      6.826319  ...   \n",
       "3             4             2           11  42.31      6.826319  ...   \n",
       "4             3             3            9  36.00      6.826319  ...   \n",
       "5             5             2           32  34.04      6.826319  ...   \n",
       "6            18            20           48  32.43      6.826319  ...   \n",
       "7             1             3            6  33.33      6.826319  ...   \n",
       "8             0             0            0   0.00      6.826319  ...   \n",
       "9             2             0            3  37.50      6.826319  ...   \n",
       "\n",
       "   PrevTotalParticipants  PrevGoldMedals  PrevSilverMedals  PrevBronzeMedals  \\\n",
       "0                      0               0                 0                 0   \n",
       "1                      0               0                 0                 0   \n",
       "2                      0               0                 0                 0   \n",
       "3                      0               0                 0                 0   \n",
       "4                      0               0                 0                 0   \n",
       "5                      0               0                 0                 0   \n",
       "6                      0               0                 0                 0   \n",
       "7                      0               0                 0                 0   \n",
       "8                      0               0                 0                 0   \n",
       "9                      0               0                 0                 0   \n",
       "\n",
       "   PrevTotalMedals  PrevWinterTotalParticipants  PrevWinterGoldMedals  \\\n",
       "0                0                            0                     0   \n",
       "1                0                            0                     0   \n",
       "2                0                            0                     0   \n",
       "3                0                            0                     0   \n",
       "4                0                            0                     0   \n",
       "5                0                            0                     0   \n",
       "6                0                            0                     0   \n",
       "7                0                            0                     0   \n",
       "8                0                            0                     0   \n",
       "9                0                            0                     0   \n",
       "\n",
       "   PrevWinterSilverMedals  PrevWinterBronzeMedals  PrevWinterTotalMedals  \n",
       "0                       0                       0                      0  \n",
       "1                       0                       0                      0  \n",
       "2                       0                       0                      0  \n",
       "3                       0                       0                      0  \n",
       "4                       0                       0                      0  \n",
       "5                       0                       0                      0  \n",
       "6                       0                       0                      0  \n",
       "7                       0                       0                      0  \n",
       "8                       0                       0                      0  \n",
       "9                       0                       0                      0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summer_dataset = pd.read_csv(\"Generated Dataset/summer_dataset.csv\")\n",
    "summer_dataset = summer_dataset.drop([\"Unnamed: 0\"], axis = 1)\n",
    "# log the GDP and Population\n",
    "summer_dataset[\"PopulationSize\"] = np.log(summer_dataset[\"PopulationSize\"])\n",
    "summer_dataset[\"GDPPerCapita\"] = np.log(summer_dataset[\"GDPPerCapita\"])\n",
    "summer_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AHO', 'ANZ', 'CRT', 'MHL', 'MNE', 'NFL', 'TPE', 'TUV', 'UNK']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the data is too few (less than 3), simply ignore that country\n",
    "NOC_values = dict(summer_dataset.groupby(\"CountryCode\")[\"CountryCode\"].value_counts())\n",
    "ignore_value = []\n",
    "for key in NOC_values:\n",
    "    if NOC_values[key] <= 3:\n",
    "        ignore_value.append(key[0])\n",
    "ignore_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 22)\n",
      "(2602, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>Year</th>\n",
       "      <th>OlympicsSeason</th>\n",
       "      <th>TotalParticipants</th>\n",
       "      <th>GoldMedals</th>\n",
       "      <th>SilverMedals</th>\n",
       "      <th>BronzeMedals</th>\n",
       "      <th>TotalMedals</th>\n",
       "      <th>Win%</th>\n",
       "      <th>GDPPerCapita</th>\n",
       "      <th>...</th>\n",
       "      <th>PrevTotalParticipants</th>\n",
       "      <th>PrevGoldMedals</th>\n",
       "      <th>PrevSilverMedals</th>\n",
       "      <th>PrevBronzeMedals</th>\n",
       "      <th>PrevTotalMedals</th>\n",
       "      <th>PrevWinterTotalParticipants</th>\n",
       "      <th>PrevWinterGoldMedals</th>\n",
       "      <th>PrevWinterSilverMedals</th>\n",
       "      <th>PrevWinterBronzeMedals</th>\n",
       "      <th>PrevWinterTotalMedals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUS</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUT</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>62.50</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEN</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>40.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRA</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>42.31</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBR</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>36.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GER</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>34.04</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRE</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>48</td>\n",
       "      <td>32.43</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HUN</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>33.33</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ITA</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SUI</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>37.50</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SWE</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>USA</td>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>74.07</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ARG</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AUS</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>100.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AUT</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>23.08</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BEL</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>43</td>\n",
       "      <td>43.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BOH</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>33.33</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BRA</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CAN</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28.57</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>COL</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>6.826319</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CountryCode  Year  OlympicsSeason  TotalParticipants  GoldMedals  \\\n",
       "0          AUS  1896               1                  5           2   \n",
       "1          AUT  1896               1                  8           2   \n",
       "2          DEN  1896               1                 15           1   \n",
       "3          FRA  1896               1                 26           5   \n",
       "4          GBR  1896               1                 25           3   \n",
       "5          GER  1896               1                 94          25   \n",
       "6          GRE  1896               1                148          10   \n",
       "7          HUN  1896               1                 18           2   \n",
       "8          ITA  1896               1                  1           0   \n",
       "9          SUI  1896               1                  8           1   \n",
       "10         SWE  1896               1                  5           0   \n",
       "11         USA  1896               1                 27          11   \n",
       "12         ARG  1900               2                  1           0   \n",
       "13         AUS  1900               2                  6           3   \n",
       "14         AUT  1900               2                 26           0   \n",
       "15         BEL  1900               2                100           6   \n",
       "16         BOH  1900               2                  9           0   \n",
       "17         BRA  1900               2                  3           0   \n",
       "18         CAN  1900               2                  7           1   \n",
       "19         COL  1900               2                  1           0   \n",
       "\n",
       "    SilverMedals  BronzeMedals  TotalMedals    Win%  GDPPerCapita  ...  \\\n",
       "0              0             1            3   60.00      6.826319  ...   \n",
       "1              1             2            5   62.50      6.826319  ...   \n",
       "2              2             3            6   40.00      6.826319  ...   \n",
       "3              4             2           11   42.31      6.826319  ...   \n",
       "4              3             3            9   36.00      6.826319  ...   \n",
       "5              5             2           32   34.04      6.826319  ...   \n",
       "6             18            20           48   32.43      6.826319  ...   \n",
       "7              1             3            6   33.33      6.826319  ...   \n",
       "8              0             0            0    0.00      6.826319  ...   \n",
       "9              2             0            3   37.50      6.826319  ...   \n",
       "10             0             0            0    0.00      6.826319  ...   \n",
       "11             7             2           20   74.07      6.826319  ...   \n",
       "12             0             0            0    0.00      6.826319  ...   \n",
       "13             0             3            6  100.00      6.826319  ...   \n",
       "14             3             3            6   23.08      6.826319  ...   \n",
       "15            24            13           43   43.00      6.826319  ...   \n",
       "16             1             2            3   33.33      6.826319  ...   \n",
       "17             0             0            0    0.00      6.826319  ...   \n",
       "18             0             1            2   28.57      6.826319  ...   \n",
       "19             1             0            1  100.00      6.826319  ...   \n",
       "\n",
       "    PrevTotalParticipants  PrevGoldMedals  PrevSilverMedals  PrevBronzeMedals  \\\n",
       "0                       0               0                 0                 0   \n",
       "1                       0               0                 0                 0   \n",
       "2                       0               0                 0                 0   \n",
       "3                       0               0                 0                 0   \n",
       "4                       0               0                 0                 0   \n",
       "5                       0               0                 0                 0   \n",
       "6                       0               0                 0                 0   \n",
       "7                       0               0                 0                 0   \n",
       "8                       0               0                 0                 0   \n",
       "9                       0               0                 0                 0   \n",
       "10                      0               0                 0                 0   \n",
       "11                      0               0                 0                 0   \n",
       "12                      0               0                 0                 0   \n",
       "13                      5               2                 0                 1   \n",
       "14                      8               2                 1                 2   \n",
       "15                      0               0                 0                 0   \n",
       "16                      0               0                 0                 0   \n",
       "17                      0               0                 0                 0   \n",
       "18                      0               0                 0                 0   \n",
       "19                      0               0                 0                 0   \n",
       "\n",
       "    PrevTotalMedals  PrevWinterTotalParticipants  PrevWinterGoldMedals  \\\n",
       "0                 0                            0                     0   \n",
       "1                 0                            0                     0   \n",
       "2                 0                            0                     0   \n",
       "3                 0                            0                     0   \n",
       "4                 0                            0                     0   \n",
       "5                 0                            0                     0   \n",
       "6                 0                            0                     0   \n",
       "7                 0                            0                     0   \n",
       "8                 0                            0                     0   \n",
       "9                 0                            0                     0   \n",
       "10                0                            0                     0   \n",
       "11                0                            0                     0   \n",
       "12                0                            0                     0   \n",
       "13                3                            0                     0   \n",
       "14                5                            0                     0   \n",
       "15                0                            0                     0   \n",
       "16                0                            0                     0   \n",
       "17                0                            0                     0   \n",
       "18                0                            0                     0   \n",
       "19                0                            0                     0   \n",
       "\n",
       "    PrevWinterSilverMedals  PrevWinterBronzeMedals  PrevWinterTotalMedals  \n",
       "0                        0                       0                      0  \n",
       "1                        0                       0                      0  \n",
       "2                        0                       0                      0  \n",
       "3                        0                       0                      0  \n",
       "4                        0                       0                      0  \n",
       "5                        0                       0                      0  \n",
       "6                        0                       0                      0  \n",
       "7                        0                       0                      0  \n",
       "8                        0                       0                      0  \n",
       "9                        0                       0                      0  \n",
       "10                       0                       0                      0  \n",
       "11                       0                       0                      0  \n",
       "12                       0                       0                      0  \n",
       "13                       0                       0                      0  \n",
       "14                       0                       0                      0  \n",
       "15                       0                       0                      0  \n",
       "16                       0                       0                      0  \n",
       "17                       0                       0                      0  \n",
       "18                       0                       0                      0  \n",
       "19                       0                       0                      0  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data is not enough to split the validation set\n",
    "# only extract the last year as the test dataset\n",
    "test = summer_dataset.groupby(\"CountryCode\").apply(lambda x: x.iloc[len(x)-1:])\n",
    "test = pd.DataFrame(test[(test[\"TotalParticipants\"] != 0) & (~(test[\"CountryCode\"].isin(ignore_value)))]).reset_index(drop = True)\n",
    "print(test.shape)\n",
    "NOC_neat = list(test[\"CountryCode\"].unique())\n",
    "NOC_neat.sort()\n",
    "train = pd.DataFrame(summer_dataset[(summer_dataset[\"CountryCode\"].isin(NOC_neat)) & (summer_dataset[\"Year\"] != 2016)]).reset_index(drop = True)\n",
    "print(train.shape)\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\"HostCity\",\"GDPPerCapita\",\"PopulationSize\",\"TotalParticipants\", \"PrevTotalParticipants\", \"PrevGoldMedals\", \"PrevSilverMedals\", \"PrevBronzeMedals\", \"PrevTotalMedals\",\\\n",
    "                      \"PrevWinterTotalParticipants\", \"PrevWinterGoldMedals\", \"PrevWinterSilverMedals\", \"PrevWinterBronzeMedals\", \"PrevWinterTotalMedals\"]\n",
    "y_train = pd.DataFrame(train[\"TotalMedals\"])\n",
    "X_train = pd.DataFrame(train[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add CallBakcs: early-stopping: stop if there is no improvement on loss more than 100 epochs\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='loss', patience=100, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2602/2602 [==============================] - 1s 230us/step - loss: 2102.1910\n",
      "Epoch 2/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 893.8837\n",
      "Epoch 3/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 775.3497\n",
      "Epoch 4/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 578.9089\n",
      "Epoch 5/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 561.3024\n",
      "Epoch 6/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 471.7766\n",
      "Epoch 7/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 511.3602\n",
      "Epoch 8/1000\n",
      "2602/2602 [==============================] - 0s 84us/step - loss: 597.6209\n",
      "Epoch 9/1000\n",
      "2602/2602 [==============================] - 0s 161us/step - loss: 419.2560\n",
      "Epoch 10/1000\n",
      "2602/2602 [==============================] - 0s 119us/step - loss: 428.3422\n",
      "Epoch 11/1000\n",
      "2602/2602 [==============================] - 0s 87us/step - loss: 388.5250\n",
      "Epoch 12/1000\n",
      "2602/2602 [==============================] - 0s 67us/step - loss: 463.2112A: 0s - loss: 538.16\n",
      "Epoch 13/1000\n",
      "2602/2602 [==============================] - 0s 67us/step - loss: 460.2752\n",
      "Epoch 14/1000\n",
      "2602/2602 [==============================] - 0s 63us/step - loss: 361.1585\n",
      "Epoch 15/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 440.7320\n",
      "Epoch 16/1000\n",
      "2602/2602 [==============================] - 0s 56us/step - loss: 373.4147\n",
      "Epoch 17/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 384.9141\n",
      "Epoch 18/1000\n",
      "2602/2602 [==============================] - 0s 70us/step - loss: 394.1826\n",
      "Epoch 19/1000\n",
      "2602/2602 [==============================] - 0s 90us/step - loss: 355.6278\n",
      "Epoch 20/1000\n",
      "2602/2602 [==============================] - 0s 62us/step - loss: 360.5611\n",
      "Epoch 21/1000\n",
      "2602/2602 [==============================] - 0s 55us/step - loss: 362.0569\n",
      "Epoch 22/1000\n",
      "2602/2602 [==============================] - 0s 65us/step - loss: 360.8025\n",
      "Epoch 23/1000\n",
      "2602/2602 [==============================] - 0s 68us/step - loss: 349.7259\n",
      "Epoch 24/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 397.3721\n",
      "Epoch 25/1000\n",
      "2602/2602 [==============================] - 0s 68us/step - loss: 389.5968\n",
      "Epoch 26/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 409.0222\n",
      "Epoch 27/1000\n",
      "2602/2602 [==============================] - 0s 84us/step - loss: 329.0171\n",
      "Epoch 28/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 301.8941\n",
      "Epoch 29/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 334.5751\n",
      "Epoch 30/1000\n",
      "2602/2602 [==============================] - 0s 58us/step - loss: 309.4285\n",
      "Epoch 31/1000\n",
      "2602/2602 [==============================] - 0s 56us/step - loss: 368.6206\n",
      "Epoch 32/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 327.5874\n",
      "Epoch 33/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 331.8960\n",
      "Epoch 34/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 318.5594\n",
      "Epoch 35/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 343.6176\n",
      "Epoch 36/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 304.4740\n",
      "Epoch 37/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 294.8815\n",
      "Epoch 38/1000\n",
      "2602/2602 [==============================] - 0s 56us/step - loss: 272.4817\n",
      "Epoch 39/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 299.5323\n",
      "Epoch 40/1000\n",
      "2602/2602 [==============================] - 0s 57us/step - loss: 329.4089\n",
      "Epoch 41/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 300.0447\n",
      "Epoch 42/1000\n",
      "2602/2602 [==============================] - 0s 57us/step - loss: 286.5131\n",
      "Epoch 43/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 321.9666\n",
      "Epoch 44/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 314.4885\n",
      "Epoch 45/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 309.2485\n",
      "Epoch 46/1000\n",
      "2602/2602 [==============================] - 0s 55us/step - loss: 326.1342\n",
      "Epoch 47/1000\n",
      "2602/2602 [==============================] - 0s 64us/step - loss: 269.8097\n",
      "Epoch 48/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 273.8160\n",
      "Epoch 49/1000\n",
      "2602/2602 [==============================] - 0s 65us/step - loss: 297.1221\n",
      "Epoch 50/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 381.6655\n",
      "Epoch 51/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 284.5418\n",
      "Epoch 52/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 322.1948\n",
      "Epoch 53/1000\n",
      "2602/2602 [==============================] - 0s 70us/step - loss: 287.5504\n",
      "Epoch 54/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 283.5345\n",
      "Epoch 55/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 300.2762\n",
      "Epoch 56/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 254.0306\n",
      "Epoch 57/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 269.9910\n",
      "Epoch 58/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 251.5250\n",
      "Epoch 59/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 275.2242\n",
      "Epoch 60/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 256.5525\n",
      "Epoch 61/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 310.8781\n",
      "Epoch 62/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 265.4657\n",
      "Epoch 63/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 275.5579\n",
      "Epoch 64/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 291.3324\n",
      "Epoch 65/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 300.5190\n",
      "Epoch 66/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 300.1994\n",
      "Epoch 67/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 264.7934\n",
      "Epoch 68/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 259.2871\n",
      "Epoch 69/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 271.8000\n",
      "Epoch 70/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 385.3245\n",
      "Epoch 71/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 289.7216\n",
      "Epoch 72/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 244.0551\n",
      "Epoch 73/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 243.6045\n",
      "Epoch 74/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 265.2460\n",
      "Epoch 75/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 265.7163: 0s - loss: 245.271\n",
      "Epoch 76/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 295.4825\n",
      "Epoch 77/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 289.8058\n",
      "Epoch 78/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 262.9839\n",
      "Epoch 79/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 335.8862\n",
      "Epoch 80/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 277.2605\n",
      "Epoch 81/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 286.1850\n",
      "Epoch 82/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 266.0262\n",
      "Epoch 83/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 269.6741\n",
      "Epoch 84/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 309.0797\n",
      "Epoch 85/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 331.9995\n",
      "Epoch 86/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 255.5404\n",
      "Epoch 87/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 262.6274\n",
      "Epoch 88/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 259.9874\n",
      "Epoch 89/1000\n",
      "2602/2602 [==============================] - 0s 60us/step - loss: 282.2050\n",
      "Epoch 90/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 286.3466\n",
      "Epoch 91/1000\n",
      "2602/2602 [==============================] - 0s 85us/step - loss: 242.6905\n",
      "Epoch 92/1000\n",
      "2602/2602 [==============================] - 0s 69us/step - loss: 251.4042\n",
      "Epoch 93/1000\n",
      "2602/2602 [==============================] - 0s 68us/step - loss: 258.3406\n",
      "Epoch 94/1000\n",
      "2602/2602 [==============================] - 0s 92us/step - loss: 241.6777\n",
      "Epoch 95/1000\n",
      "2602/2602 [==============================] - 0s 67us/step - loss: 314.4175\n",
      "Epoch 96/1000\n",
      "2602/2602 [==============================] - 0s 81us/step - loss: 265.2156\n",
      "Epoch 97/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 314.1885\n",
      "Epoch 98/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 247.2420\n",
      "Epoch 99/1000\n",
      "2602/2602 [==============================] - 0s 55us/step - loss: 341.3342\n",
      "Epoch 100/1000\n",
      "2602/2602 [==============================] - 0s 89us/step - loss: 231.0640\n",
      "Epoch 101/1000\n",
      "2602/2602 [==============================] - 0s 66us/step - loss: 282.4880\n",
      "Epoch 102/1000\n",
      "2602/2602 [==============================] - 0s 62us/step - loss: 302.9923\n",
      "Epoch 103/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 276.2492\n",
      "Epoch 104/1000\n",
      "2602/2602 [==============================] - 0s 53us/step - loss: 265.9308\n",
      "Epoch 105/1000\n",
      "2602/2602 [==============================] - 0s 62us/step - loss: 282.6686\n",
      "Epoch 106/1000\n",
      "2602/2602 [==============================] - 0s 57us/step - loss: 322.2786\n",
      "Epoch 107/1000\n",
      "2602/2602 [==============================] - 0s 71us/step - loss: 235.1182\n",
      "Epoch 108/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 252.0854\n",
      "Epoch 109/1000\n",
      "2602/2602 [==============================] - ETA: 0s - loss: 315.876 - 0s 38us/step - loss: 272.7706\n",
      "Epoch 110/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 237.9155\n",
      "Epoch 111/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 281.5938\n",
      "Epoch 112/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 220.3907\n",
      "Epoch 113/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 283.5188\n",
      "Epoch 114/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 239.3324\n",
      "Epoch 115/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 254.3148\n",
      "Epoch 116/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 261.8582\n",
      "Epoch 117/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 317.9895\n",
      "Epoch 118/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 384.4410\n",
      "Epoch 119/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 317.0920\n",
      "Epoch 120/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 249.0523\n",
      "Epoch 121/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 273.2835\n",
      "Epoch 122/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 241.6550\n",
      "Epoch 123/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 326.3968: 0s - loss: 197.45\n",
      "Epoch 124/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 304.8189\n",
      "Epoch 125/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 258.5761\n",
      "Epoch 126/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 260.3099\n",
      "Epoch 127/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 213.3326\n",
      "Epoch 128/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 344.8296\n",
      "Epoch 129/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 258.4495\n",
      "Epoch 130/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 292.4573\n",
      "Epoch 131/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 240.4213\n",
      "Epoch 132/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 235.2926\n",
      "Epoch 133/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 248.2529\n",
      "Epoch 134/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 213.5442\n",
      "Epoch 135/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 313.0236\n",
      "Epoch 136/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 263.3302\n",
      "Epoch 137/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 232.6031\n",
      "Epoch 138/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 251.4419\n",
      "Epoch 139/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 253.2442\n",
      "Epoch 140/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 230.1932\n",
      "Epoch 141/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 236.2819\n",
      "Epoch 142/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 240.2959\n",
      "Epoch 143/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 288.3473\n",
      "Epoch 144/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 247.0875\n",
      "Epoch 145/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 248.1035\n",
      "Epoch 146/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 349.3347\n",
      "Epoch 147/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 196.2016\n",
      "Epoch 148/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 235.3786\n",
      "Epoch 149/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 230.3040\n",
      "Epoch 150/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 222.0581\n",
      "Epoch 151/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 245.3375\n",
      "Epoch 152/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 207.9875\n",
      "Epoch 153/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 244.0193\n",
      "Epoch 154/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 189.2277\n",
      "Epoch 155/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 248.3456\n",
      "Epoch 156/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 212.7106\n",
      "Epoch 157/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 237.6735\n",
      "Epoch 158/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 234.7223\n",
      "Epoch 159/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 267.4053\n",
      "Epoch 160/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 284.2975\n",
      "Epoch 161/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 229.9684\n",
      "Epoch 162/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 316.1336\n",
      "Epoch 163/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 248.1574\n",
      "Epoch 164/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 257.3456\n",
      "Epoch 165/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 246.0616\n",
      "Epoch 166/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 213.5818\n",
      "Epoch 167/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 204.4650\n",
      "Epoch 168/1000\n",
      "2602/2602 [==============================] - 0s 72us/step - loss: 214.2542\n",
      "Epoch 169/1000\n",
      "2602/2602 [==============================] - 0s 61us/step - loss: 219.3711\n",
      "Epoch 170/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 307.0054\n",
      "Epoch 171/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 252.9732\n",
      "Epoch 172/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 215.7041\n",
      "Epoch 173/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 220.1530\n",
      "Epoch 174/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 282.3843\n",
      "Epoch 175/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 275.6282\n",
      "Epoch 176/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 204.6809\n",
      "Epoch 177/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 232.7078\n",
      "Epoch 178/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 247.1269\n",
      "Epoch 179/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 219.1150\n",
      "Epoch 180/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 209.1460\n",
      "Epoch 181/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 218.1008\n",
      "Epoch 182/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602/2602 [==============================] - 0s 38us/step - loss: 255.7071\n",
      "Epoch 183/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 222.4148\n",
      "Epoch 184/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 257.7593\n",
      "Epoch 185/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 294.4941\n",
      "Epoch 186/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 229.4248\n",
      "Epoch 187/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 219.9502\n",
      "Epoch 188/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 254.1438\n",
      "Epoch 189/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 251.0710\n",
      "Epoch 190/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 265.2393\n",
      "Epoch 191/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 227.4666\n",
      "Epoch 192/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 222.1227\n",
      "Epoch 193/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 211.5015\n",
      "Epoch 194/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 208.9473\n",
      "Epoch 195/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 201.3515\n",
      "Epoch 196/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 251.6611\n",
      "Epoch 197/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 201.4384\n",
      "Epoch 198/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 211.9367\n",
      "Epoch 199/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 214.4343\n",
      "Epoch 200/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 243.7135\n",
      "Epoch 201/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 266.0284\n",
      "Epoch 202/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 250.0320\n",
      "Epoch 203/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 285.5744\n",
      "Epoch 204/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 227.2455\n",
      "Epoch 205/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 220.0746\n",
      "Epoch 206/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 257.4983\n",
      "Epoch 207/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 224.7870\n",
      "Epoch 208/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 236.3364\n",
      "Epoch 209/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 223.6752\n",
      "Epoch 210/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 253.8380\n",
      "Epoch 211/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 245.3883\n",
      "Epoch 212/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 218.3394\n",
      "Epoch 213/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 214.6057\n",
      "Epoch 214/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 311.8168\n",
      "Epoch 215/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 310.3729\n",
      "Epoch 216/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 196.8791\n",
      "Epoch 217/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 235.6827\n",
      "Epoch 218/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 243.7868\n",
      "Epoch 219/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 200.9148\n",
      "Epoch 220/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 323.5087\n",
      "Epoch 221/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 238.7727\n",
      "Epoch 222/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 259.4310\n",
      "Epoch 223/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 236.6462\n",
      "Epoch 224/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 228.1264\n",
      "Epoch 225/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 218.7581\n",
      "Epoch 226/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 270.6718\n",
      "Epoch 227/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 280.7489\n",
      "Epoch 228/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 318.4313\n",
      "Epoch 229/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 196.5441\n",
      "Epoch 230/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 299.4307\n",
      "Epoch 231/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 223.0140\n",
      "Epoch 232/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 207.0358\n",
      "Epoch 233/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 256.3274\n",
      "Epoch 234/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 267.1967\n",
      "Epoch 235/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 254.5598\n",
      "Epoch 236/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 203.5174\n",
      "Epoch 237/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 230.2296\n",
      "Epoch 238/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 234.3526\n",
      "Epoch 239/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 267.3370\n",
      "Epoch 240/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 215.5579\n",
      "Epoch 241/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 171.6807\n",
      "Epoch 242/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 213.7855\n",
      "Epoch 243/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 209.8295\n",
      "Epoch 244/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 266.4496\n",
      "Epoch 245/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 288.1616\n",
      "Epoch 246/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 211.0142\n",
      "Epoch 247/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 287.8331\n",
      "Epoch 248/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 223.1366\n",
      "Epoch 249/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 198.2722\n",
      "Epoch 250/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 263.3515\n",
      "Epoch 251/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 296.3544\n",
      "Epoch 252/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 231.9221\n",
      "Epoch 253/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 216.2317\n",
      "Epoch 254/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 262.7862\n",
      "Epoch 255/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 200.3732\n",
      "Epoch 256/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 306.0614\n",
      "Epoch 257/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 237.2808\n",
      "Epoch 258/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 210.6164\n",
      "Epoch 259/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 193.1972\n",
      "Epoch 260/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 282.0029\n",
      "Epoch 261/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 207.0944\n",
      "Epoch 262/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 195.0806\n",
      "Epoch 263/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 252.9754\n",
      "Epoch 264/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 247.2119\n",
      "Epoch 265/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 286.7771\n",
      "Epoch 266/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 276.8136\n",
      "Epoch 267/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 304.4822\n",
      "Epoch 268/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 224.0087\n",
      "Epoch 269/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 224.4948\n",
      "Epoch 270/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 211.8033\n",
      "Epoch 271/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 231.8427\n",
      "Epoch 272/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 226.1334\n",
      "Epoch 273/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 216.7037\n",
      "Epoch 274/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 200.7954\n",
      "Epoch 275/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 224.1277\n",
      "Epoch 276/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 216.6005\n",
      "Epoch 277/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 229.0978\n",
      "Epoch 278/1000\n",
      "2602/2602 [==============================] - 0s 33us/step - loss: 235.9294\n",
      "Epoch 279/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 206.6938\n",
      "Epoch 280/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 241.6411\n",
      "Epoch 281/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 234.4801\n",
      "Epoch 282/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 214.5782\n",
      "Epoch 283/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 224.8591\n",
      "Epoch 284/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 212.2993\n",
      "Epoch 285/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 293.9443\n",
      "Epoch 286/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 246.8617\n",
      "Epoch 287/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 219.4087\n",
      "Epoch 288/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 291.7521\n",
      "Epoch 289/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 197.8174\n",
      "Epoch 290/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 212.9558\n",
      "Epoch 291/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 218.0919\n",
      "Epoch 292/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 201.8480\n",
      "Epoch 293/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 218.8032\n",
      "Epoch 294/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 261.0032\n",
      "Epoch 295/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 324.2727\n",
      "Epoch 296/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 255.0984\n",
      "Epoch 297/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 254.2839\n",
      "Epoch 298/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 241.0297\n",
      "Epoch 299/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 223.0491\n",
      "Epoch 300/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 204.1184\n",
      "Epoch 301/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 235.2222\n",
      "Epoch 302/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 304.2866\n",
      "Epoch 303/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 236.1041\n",
      "Epoch 304/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 222.4992\n",
      "Epoch 305/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 272.9074\n",
      "Epoch 306/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 223.4265\n",
      "Epoch 307/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 209.6445\n",
      "Epoch 308/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 259.6652\n",
      "Epoch 309/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 206.3106\n",
      "Epoch 310/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 185.4505\n",
      "Epoch 311/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 231.5679\n",
      "Epoch 312/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 249.3256\n",
      "Epoch 313/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 193.5272\n",
      "Epoch 314/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 187.9707\n",
      "Epoch 315/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 225.7335\n",
      "Epoch 316/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 201.4551\n",
      "Epoch 317/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 275.4519\n",
      "Epoch 318/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 241.0784\n",
      "Epoch 319/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 223.4996\n",
      "Epoch 320/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 202.6873\n",
      "Epoch 321/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 316.1448\n",
      "Epoch 322/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 262.0765\n",
      "Epoch 323/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 305.5986\n",
      "Epoch 324/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 270.6220\n",
      "Epoch 325/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 234.9959\n",
      "Epoch 326/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 309.3399\n",
      "Epoch 327/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 226.0581\n",
      "Epoch 328/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 188.2328\n",
      "Epoch 329/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 193.5911\n",
      "Epoch 330/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 207.1601\n",
      "Epoch 331/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 256.2585\n",
      "Epoch 332/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 218.8653\n",
      "Epoch 333/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 220.0904\n",
      "Epoch 334/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 231.8202\n",
      "Epoch 335/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 246.7216\n",
      "Epoch 336/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 254.9284\n",
      "Epoch 337/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 170.9812\n",
      "Epoch 338/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 227.3656\n",
      "Epoch 339/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 199.9087\n",
      "Epoch 340/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 215.5777\n",
      "Epoch 341/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 182.8918\n",
      "Epoch 342/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 193.6737\n",
      "Epoch 343/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 257.6925\n",
      "Epoch 344/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 203.0202\n",
      "Epoch 345/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 238.4371\n",
      "Epoch 346/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 216.4756\n",
      "Epoch 347/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 294.5769\n",
      "Epoch 348/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 223.1893\n",
      "Epoch 349/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 254.9680\n",
      "Epoch 350/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 211.4803\n",
      "Epoch 351/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 203.2205\n",
      "Epoch 352/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 268.1110\n",
      "Epoch 353/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 202.1821\n",
      "Epoch 354/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 205.3398\n",
      "Epoch 355/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 216.6461\n",
      "Epoch 356/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 271.0142\n",
      "Epoch 357/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 250.6354\n",
      "Epoch 358/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 297.6492\n",
      "Epoch 359/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 212.2836\n",
      "Epoch 360/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 221.2449\n",
      "Epoch 361/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 220.3570\n",
      "Epoch 362/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 233.5512\n",
      "Epoch 363/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 236.2477\n",
      "Epoch 364/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602/2602 [==============================] - 0s 38us/step - loss: 198.4472\n",
      "Epoch 365/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 205.3436\n",
      "Epoch 366/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 194.5374\n",
      "Epoch 367/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 245.4725\n",
      "Epoch 368/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 183.7538\n",
      "Epoch 369/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 200.1555\n",
      "Epoch 370/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 197.7625\n",
      "Epoch 371/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 211.3476\n",
      "Epoch 372/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 210.3474\n",
      "Epoch 373/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 222.8357\n",
      "Epoch 374/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 229.1459\n",
      "Epoch 375/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 205.7757\n",
      "Epoch 376/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 256.0960\n",
      "Epoch 377/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 219.5006\n",
      "Epoch 378/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 266.3228\n",
      "Epoch 379/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 214.1442\n",
      "Epoch 380/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 178.4165\n",
      "Epoch 381/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 277.8885\n",
      "Epoch 382/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 308.7832\n",
      "Epoch 383/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 243.1481\n",
      "Epoch 384/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 259.8218\n",
      "Epoch 385/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 202.0741\n",
      "Epoch 386/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 228.7297\n",
      "Epoch 387/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 210.2155\n",
      "Epoch 388/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 181.8673\n",
      "Epoch 389/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 191.7490\n",
      "Epoch 390/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 235.1202\n",
      "Epoch 391/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 202.6962\n",
      "Epoch 392/1000\n",
      "2602/2602 [==============================] - 0s 34us/step - loss: 185.3111\n",
      "Epoch 393/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 199.1929\n",
      "Epoch 394/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 213.4727\n",
      "Epoch 395/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 234.9063\n",
      "Epoch 396/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 192.1455\n",
      "Epoch 397/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 233.3555\n",
      "Epoch 398/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 205.2215\n",
      "Epoch 399/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 226.7730\n",
      "Epoch 400/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 187.4066\n",
      "Epoch 401/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 196.5946\n",
      "Epoch 402/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 234.9169\n",
      "Epoch 403/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 173.1531\n",
      "Epoch 404/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 212.3312\n",
      "Epoch 405/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 221.6253\n",
      "Epoch 406/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 227.8576\n",
      "Epoch 407/1000\n",
      "2602/2602 [==============================] - 0s 35us/step - loss: 229.9999\n",
      "Epoch 408/1000\n",
      "2602/2602 [==============================] - 0s 36us/step - loss: 218.6470\n",
      "Epoch 409/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 195.9514\n",
      "Epoch 410/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 268.8047\n",
      "Epoch 411/1000\n",
      "2602/2602 [==============================] - 0s 63us/step - loss: 182.7655\n",
      "Epoch 412/1000\n",
      "2602/2602 [==============================] - 0s 57us/step - loss: 204.2085\n",
      "Epoch 413/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 207.3107\n",
      "Epoch 414/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 228.5517\n",
      "Epoch 415/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 306.3238\n",
      "Epoch 416/1000\n",
      "2602/2602 [==============================] - ETA: 0s - loss: 243.595 - 0s 71us/step - loss: 246.2004\n",
      "Epoch 417/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 214.1321\n",
      "Epoch 418/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 213.9278\n",
      "Epoch 419/1000\n",
      "2602/2602 [==============================] - 0s 58us/step - loss: 210.0811\n",
      "Epoch 420/1000\n",
      "2602/2602 [==============================] - 0s 63us/step - loss: 259.6816\n",
      "Epoch 421/1000\n",
      "2602/2602 [==============================] - 0s 82us/step - loss: 225.6160\n",
      "Epoch 422/1000\n",
      "2602/2602 [==============================] - 0s 67us/step - loss: 235.4774\n",
      "Epoch 423/1000\n",
      "2602/2602 [==============================] - 0s 69us/step - loss: 212.9322\n",
      "Epoch 424/1000\n",
      "2602/2602 [==============================] - 0s 72us/step - loss: 239.5812\n",
      "Epoch 425/1000\n",
      "2602/2602 [==============================] - 0s 96us/step - loss: 241.3741\n",
      "Epoch 426/1000\n",
      "2602/2602 [==============================] - 0s 76us/step - loss: 217.3257\n",
      "Epoch 427/1000\n",
      "2602/2602 [==============================] - 0s 72us/step - loss: 235.2436\n",
      "Epoch 428/1000\n",
      "2602/2602 [==============================] - 0s 88us/step - loss: 214.6821\n",
      "Epoch 429/1000\n",
      "2602/2602 [==============================] - 0s 85us/step - loss: 240.8016\n",
      "Epoch 430/1000\n",
      "2602/2602 [==============================] - 0s 166us/step - loss: 260.2084\n",
      "Epoch 431/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 233.8015\n",
      "Epoch 432/1000\n",
      "2602/2602 [==============================] - 0s 109us/step - loss: 236.3619\n",
      "Epoch 433/1000\n",
      "2602/2602 [==============================] - 0s 89us/step - loss: 229.5255\n",
      "Epoch 434/1000\n",
      "2602/2602 [==============================] - 0s 90us/step - loss: 254.6830\n",
      "Epoch 435/1000\n",
      "2602/2602 [==============================] - 0s 85us/step - loss: 203.0791: 0s - loss: 203.76\n",
      "Epoch 436/1000\n",
      "2602/2602 [==============================] - 0s 107us/step - loss: 267.6500\n",
      "Epoch 437/1000\n",
      "2602/2602 [==============================] - 0s 78us/step - loss: 233.4511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19446ca64e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME = \"{}-layers\".format(2)\n",
    "checkpointfunc1 = keras.callbacks.ModelCheckpoint('Keras Models/{}.model'.format(NAME), monitor='loss', verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False, mode='min', period=1)\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4, input_dim=14, activation='relu')) # 14 predictors in total\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=1000, verbose=1, callbacks=[checkpointfunc1, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors = [\"Previous Total Medals Summer\", \"Previous Gold Summer\", \"Previous Silver Summer\", \"Previous Bronze Summer\"]\n",
    "y_test = pd.DataFrame(test[\"TotalMedals\"])\n",
    "X_test = pd.DataFrame(test[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(np.array(X_test))\n",
    "y_test_pred = list(y_test_pred)\n",
    "for i in range(len(y_test_pred)):\n",
    "    if y_test_pred[i] < 0:\n",
    "        y_test_pred[i] = 0\n",
    "y_test_pred = np.round(y_test_pred).astype(\"int\")\n",
    "R2 = explained_variance_score(y_test, y_test_pred)\n",
    "MSE = mean_squared_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance (R^2) \t: 0.7750500457251938\n",
      "Mean Squared Error (MSE) \t: 205.01020408163265\n"
     ]
    }
   ],
   "source": [
    "print(\"Explained Variance (R^2) \\t:\", explained_variance_score(y_test, y_test_pred))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another version with more dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2602/2602 [==============================] - 1s 435us/step - loss: 1676.1118\n",
      "Epoch 2/1000\n",
      "2602/2602 [==============================] - 0s 76us/step - loss: 1561.3199\n",
      "Epoch 3/1000\n",
      "2602/2602 [==============================] - 0s 84us/step - loss: 1548.2599\n",
      "Epoch 4/1000\n",
      "2602/2602 [==============================] - 0s 57us/step - loss: 1534.3559\n",
      "Epoch 5/1000\n",
      "2602/2602 [==============================] - 0s 60us/step - loss: 1526.4348\n",
      "Epoch 6/1000\n",
      "2602/2602 [==============================] - 0s 123us/step - loss: 1519.4069\n",
      "Epoch 7/1000\n",
      "2602/2602 [==============================] - 0s 112us/step - loss: 1514.6575\n",
      "Epoch 8/1000\n",
      "2602/2602 [==============================] - 0s 77us/step - loss: 1510.4619\n",
      "Epoch 9/1000\n",
      "2602/2602 [==============================] - 0s 123us/step - loss: 1506.2831\n",
      "Epoch 10/1000\n",
      "2602/2602 [==============================] - 0s 105us/step - loss: 1501.8681\n",
      "Epoch 11/1000\n",
      "2602/2602 [==============================] - 0s 89us/step - loss: 1497.2661\n",
      "Epoch 12/1000\n",
      "2602/2602 [==============================] - 0s 82us/step - loss: 1492.2538\n",
      "Epoch 13/1000\n",
      "2602/2602 [==============================] - 0s 67us/step - loss: 1486.8784\n",
      "Epoch 14/1000\n",
      "2602/2602 [==============================] - 0s 66us/step - loss: 1481.2208\n",
      "Epoch 15/1000\n",
      "2602/2602 [==============================] - 0s 77us/step - loss: 1475.1989\n",
      "Epoch 16/1000\n",
      "2602/2602 [==============================] - 0s 72us/step - loss: 1468.7411\n",
      "Epoch 17/1000\n",
      "2602/2602 [==============================] - 0s 64us/step - loss: 1461.8120\n",
      "Epoch 18/1000\n",
      "2602/2602 [==============================] - 0s 61us/step - loss: 1454.4167\n",
      "Epoch 19/1000\n",
      "2602/2602 [==============================] - 0s 64us/step - loss: 1441.5482\n",
      "Epoch 20/1000\n",
      "2602/2602 [==============================] - 0s 61us/step - loss: 1412.9572\n",
      "Epoch 21/1000\n",
      "2602/2602 [==============================] - 0s 61us/step - loss: 1193.7641\n",
      "Epoch 22/1000\n",
      "2602/2602 [==============================] - 0s 65us/step - loss: 817.5734\n",
      "Epoch 23/1000\n",
      "2602/2602 [==============================] - 0s 62us/step - loss: 652.2055\n",
      "Epoch 24/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 713.4939\n",
      "Epoch 25/1000\n",
      "2602/2602 [==============================] - 0s 65us/step - loss: 547.0915\n",
      "Epoch 26/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 593.3035\n",
      "Epoch 27/1000\n",
      "2602/2602 [==============================] - 0s 67us/step - loss: 484.6449\n",
      "Epoch 28/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 565.9289\n",
      "Epoch 29/1000\n",
      "2602/2602 [==============================] - 0s 66us/step - loss: 691.4917\n",
      "Epoch 30/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 562.4680\n",
      "Epoch 31/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 538.8721\n",
      "Epoch 32/1000\n",
      "2602/2602 [==============================] - 0s 67us/step - loss: 481.5734\n",
      "Epoch 33/1000\n",
      "2602/2602 [==============================] - 0s 65us/step - loss: 480.1842\n",
      "Epoch 34/1000\n",
      "2602/2602 [==============================] - 0s 61us/step - loss: 393.4478\n",
      "Epoch 35/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 623.2093\n",
      "Epoch 36/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 631.7988\n",
      "Epoch 37/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 551.4398\n",
      "Epoch 38/1000\n",
      "2602/2602 [==============================] - 0s 55us/step - loss: 538.0387\n",
      "Epoch 39/1000\n",
      "2602/2602 [==============================] - 0s 60us/step - loss: 576.9731\n",
      "Epoch 40/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 574.3566\n",
      "Epoch 41/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 563.0987\n",
      "Epoch 42/1000\n",
      "2602/2602 [==============================] - 0s 53us/step - loss: 544.6470\n",
      "Epoch 43/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 500.6558\n",
      "Epoch 44/1000\n",
      "2602/2602 [==============================] - 0s 64us/step - loss: 521.6001\n",
      "Epoch 45/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 543.3959\n",
      "Epoch 46/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 487.4375\n",
      "Epoch 47/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 490.4844\n",
      "Epoch 48/1000\n",
      "2602/2602 [==============================] - 0s 56us/step - loss: 531.9468\n",
      "Epoch 49/1000\n",
      "2602/2602 [==============================] - 0s 56us/step - loss: 455.2212\n",
      "Epoch 50/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 477.3695\n",
      "Epoch 51/1000\n",
      "2602/2602 [==============================] - 0s 56us/step - loss: 513.5147\n",
      "Epoch 52/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 495.4234\n",
      "Epoch 53/1000\n",
      "2602/2602 [==============================] - 0s 55us/step - loss: 609.1948\n",
      "Epoch 54/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 450.0893\n",
      "Epoch 55/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 447.9319\n",
      "Epoch 56/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 669.3996\n",
      "Epoch 57/1000\n",
      "2602/2602 [==============================] - 0s 53us/step - loss: 412.7369\n",
      "Epoch 58/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 461.7295\n",
      "Epoch 59/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 518.2044\n",
      "Epoch 60/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 430.0959: 0s - loss: 358.21\n",
      "Epoch 61/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 415.6869\n",
      "Epoch 62/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 575.0953\n",
      "Epoch 63/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 565.6870\n",
      "Epoch 64/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 637.9989\n",
      "Epoch 65/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 471.7883\n",
      "Epoch 66/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 510.7013\n",
      "Epoch 67/1000\n",
      "2602/2602 [==============================] - 0s 52us/step - loss: 461.4384\n",
      "Epoch 68/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 598.1809\n",
      "Epoch 69/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 590.4577\n",
      "Epoch 70/1000\n",
      "2602/2602 [==============================] - 0s 62us/step - loss: 380.8304\n",
      "Epoch 71/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 707.4533\n",
      "Epoch 72/1000\n",
      "2602/2602 [==============================] - ETA: 0s - loss: 593.987 - 0s 66us/step - loss: 629.4978\n",
      "Epoch 73/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 570.6989\n",
      "Epoch 74/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 579.1418\n",
      "Epoch 75/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 505.5774\n",
      "Epoch 76/1000\n",
      "2602/2602 [==============================] - 0s 53us/step - loss: 605.8511\n",
      "Epoch 77/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 444.6193\n",
      "Epoch 78/1000\n",
      "2602/2602 [==============================] - 0s 54us/step - loss: 422.5193\n",
      "Epoch 79/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 520.9944\n",
      "Epoch 80/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 513.8255\n",
      "Epoch 81/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 505.4415\n",
      "Epoch 82/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 530.8698\n",
      "Epoch 83/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 516.9414\n",
      "Epoch 84/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 550.6928\n",
      "Epoch 85/1000\n",
      "2602/2602 [==============================] - 0s 53us/step - loss: 495.4546\n",
      "Epoch 86/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 584.9776\n",
      "Epoch 87/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 506.4960\n",
      "Epoch 88/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 511.7613\n",
      "Epoch 89/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 635.9166\n",
      "Epoch 90/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 436.4167\n",
      "Epoch 91/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 549.6671\n",
      "Epoch 92/1000\n",
      "2602/2602 [==============================] - 0s 51us/step - loss: 541.6885\n",
      "Epoch 93/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 456.2468\n",
      "Epoch 94/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 640.2247\n",
      "Epoch 95/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 491.0921\n",
      "Epoch 96/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 501.5214\n",
      "Epoch 97/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 493.2630\n",
      "Epoch 98/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 508.5311\n",
      "Epoch 99/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 473.7787\n",
      "Epoch 100/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 612.7998\n",
      "Epoch 101/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 449.4847\n",
      "Epoch 102/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 506.3798\n",
      "Epoch 103/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 550.1078\n",
      "Epoch 104/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 496.3726\n",
      "Epoch 105/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 537.4973\n",
      "Epoch 106/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 435.4327\n",
      "Epoch 107/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 500.5920\n",
      "Epoch 108/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 560.3886\n",
      "Epoch 109/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 527.6472\n",
      "Epoch 110/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 504.3103\n",
      "Epoch 111/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 562.8692\n",
      "Epoch 112/1000\n",
      "2602/2602 [==============================] - 0s 56us/step - loss: 376.5925\n",
      "Epoch 113/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 466.3019\n",
      "Epoch 114/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 616.5543\n",
      "Epoch 115/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 496.5516\n",
      "Epoch 116/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 534.5515\n",
      "Epoch 117/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 543.4354\n",
      "Epoch 118/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 628.8216\n",
      "Epoch 119/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 382.5039\n",
      "Epoch 120/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 518.9858\n",
      "Epoch 121/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 408.6993\n",
      "Epoch 122/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 502.7834\n",
      "Epoch 123/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 480.3189\n",
      "Epoch 124/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 500.2918\n",
      "Epoch 125/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 577.2006\n",
      "Epoch 126/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 541.1893\n",
      "Epoch 127/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 551.6277\n",
      "Epoch 128/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 475.5754\n",
      "Epoch 129/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 543.0822\n",
      "Epoch 130/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 643.8311\n",
      "Epoch 131/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 552.4622\n",
      "Epoch 132/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 588.2414\n",
      "Epoch 133/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 593.9048\n",
      "Epoch 134/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 528.3422\n",
      "Epoch 135/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 664.5432\n",
      "Epoch 136/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 588.9565\n",
      "Epoch 137/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 612.9657\n",
      "Epoch 138/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 413.8982\n",
      "Epoch 139/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 527.4663\n",
      "Epoch 140/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 462.1760\n",
      "Epoch 141/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 448.7517\n",
      "Epoch 142/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 579.3336\n",
      "Epoch 143/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 610.1254\n",
      "Epoch 144/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 387.3684\n",
      "Epoch 145/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 707.7810\n",
      "Epoch 146/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 642.8613\n",
      "Epoch 147/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 661.4560\n",
      "Epoch 148/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 493.1139\n",
      "Epoch 149/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 452.9591\n",
      "Epoch 150/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 581.4984\n",
      "Epoch 151/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 476.2745\n",
      "Epoch 152/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 474.3873\n",
      "Epoch 153/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 425.5536\n",
      "Epoch 154/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 528.0500\n",
      "Epoch 155/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 631.3987\n",
      "Epoch 156/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 624.5486\n",
      "Epoch 157/1000\n",
      "2602/2602 [==============================] - 0s 61us/step - loss: 483.3169\n",
      "Epoch 158/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 500.1579\n",
      "Epoch 159/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 564.1833\n",
      "Epoch 160/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 596.6208\n",
      "Epoch 161/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 449.4295\n",
      "Epoch 162/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 550.8219\n",
      "Epoch 163/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 494.8247\n",
      "Epoch 164/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 430.4069\n",
      "Epoch 165/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 548.0925\n",
      "Epoch 166/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 549.2685\n",
      "Epoch 167/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 468.3412\n",
      "Epoch 168/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 496.0384\n",
      "Epoch 169/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 600.2917\n",
      "Epoch 170/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 471.1568\n",
      "Epoch 171/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 558.6915\n",
      "Epoch 172/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 514.4290\n",
      "Epoch 173/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 513.6236\n",
      "Epoch 174/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 601.5006\n",
      "Epoch 175/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 455.8065\n",
      "Epoch 176/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 463.2970\n",
      "Epoch 177/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 494.0287\n",
      "Epoch 178/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 521.3568\n",
      "Epoch 179/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 477.6586\n",
      "Epoch 180/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 479.2215\n",
      "Epoch 181/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 501.2320\n",
      "Epoch 182/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 588.0627\n",
      "Epoch 183/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602/2602 [==============================] - 0s 46us/step - loss: 468.9116\n",
      "Epoch 184/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 604.0837\n",
      "Epoch 185/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 452.3469\n",
      "Epoch 186/1000\n",
      "2602/2602 [==============================] - 0s 37us/step - loss: 489.8913\n",
      "Epoch 187/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 622.8852\n",
      "Epoch 188/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 542.5166\n",
      "Epoch 189/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 525.0536\n",
      "Epoch 190/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 398.4676\n",
      "Epoch 191/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 466.3773\n",
      "Epoch 192/1000\n",
      "2602/2602 [==============================] - 0s 55us/step - loss: 375.9778\n",
      "Epoch 193/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 613.1959\n",
      "Epoch 194/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 471.4320\n",
      "Epoch 195/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 582.2517\n",
      "Epoch 196/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 515.7755\n",
      "Epoch 197/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 522.7014\n",
      "Epoch 198/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 539.6202\n",
      "Epoch 199/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 378.7465\n",
      "Epoch 200/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 591.0050\n",
      "Epoch 201/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 551.7520\n",
      "Epoch 202/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 423.1515\n",
      "Epoch 203/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 658.1072\n",
      "Epoch 204/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 492.8499\n",
      "Epoch 205/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 556.4730\n",
      "Epoch 206/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 406.8258\n",
      "Epoch 207/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 416.3757\n",
      "Epoch 208/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 588.9918\n",
      "Epoch 209/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 405.9248\n",
      "Epoch 210/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 626.5854\n",
      "Epoch 211/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 643.5815\n",
      "Epoch 212/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 607.95750s - loss: 407.251\n",
      "Epoch 213/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 413.9312\n",
      "Epoch 214/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 550.3772\n",
      "Epoch 215/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 648.7180\n",
      "Epoch 216/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 588.1583\n",
      "Epoch 217/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 549.7383\n",
      "Epoch 218/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 448.9762\n",
      "Epoch 219/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 533.9857\n",
      "Epoch 220/1000\n",
      "2602/2602 [==============================] - 0s 53us/step - loss: 463.7677\n",
      "Epoch 221/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 607.8888\n",
      "Epoch 222/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 436.5834\n",
      "Epoch 223/1000\n",
      "2602/2602 [==============================] - 0s 59us/step - loss: 545.8950\n",
      "Epoch 224/1000\n",
      "2602/2602 [==============================] - 0s 79us/step - loss: 537.7619\n",
      "Epoch 225/1000\n",
      "2602/2602 [==============================] - 0s 63us/step - loss: 466.0102\n",
      "Epoch 226/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 502.2983\n",
      "Epoch 227/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 471.6560\n",
      "Epoch 228/1000\n",
      "2602/2602 [==============================] - 0s 57us/step - loss: 353.3944\n",
      "Epoch 229/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 436.4850\n",
      "Epoch 230/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 444.5567\n",
      "Epoch 231/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 475.4546\n",
      "Epoch 232/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 526.0431\n",
      "Epoch 233/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 502.0787\n",
      "Epoch 234/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 520.1444\n",
      "Epoch 235/1000\n",
      "2602/2602 [==============================] - 0s 53us/step - loss: 330.9952\n",
      "Epoch 236/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 533.9226\n",
      "Epoch 237/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 466.5242\n",
      "Epoch 238/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 497.5308\n",
      "Epoch 239/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 470.7114\n",
      "Epoch 240/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 435.5003\n",
      "Epoch 241/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 551.2322\n",
      "Epoch 242/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 414.6269\n",
      "Epoch 243/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 373.4611\n",
      "Epoch 244/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 518.4645\n",
      "Epoch 245/1000\n",
      "2602/2602 [==============================] - 0s 50us/step - loss: 470.5485\n",
      "Epoch 246/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 514.9659\n",
      "Epoch 247/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 443.3480\n",
      "Epoch 248/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 586.4003\n",
      "Epoch 249/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 467.6946\n",
      "Epoch 250/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 562.3169\n",
      "Epoch 251/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 537.5225\n",
      "Epoch 252/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 487.7130\n",
      "Epoch 253/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 631.6728\n",
      "Epoch 254/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 404.8583\n",
      "Epoch 255/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 461.7234\n",
      "Epoch 256/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 474.7999\n",
      "Epoch 257/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 552.4028\n",
      "Epoch 258/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 605.8154\n",
      "Epoch 259/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 542.1154\n",
      "Epoch 260/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 502.6339\n",
      "Epoch 261/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 427.1999\n",
      "Epoch 262/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 441.9850\n",
      "Epoch 263/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 478.7632: 0s - loss: 302.34\n",
      "Epoch 264/1000\n",
      "2602/2602 [==============================] - 0s 49us/step - loss: 417.4197\n",
      "Epoch 265/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 561.2328\n",
      "Epoch 266/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 549.3935\n",
      "Epoch 267/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 638.2285\n",
      "Epoch 268/1000\n",
      "2602/2602 [==============================] - 0s 48us/step - loss: 455.7647\n",
      "Epoch 269/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 468.4532\n",
      "Epoch 270/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 441.0217\n",
      "Epoch 271/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 538.9632\n",
      "Epoch 272/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 533.7008\n",
      "Epoch 273/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 451.8088\n",
      "Epoch 274/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 371.2342\n",
      "Epoch 275/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 571.3596\n",
      "Epoch 276/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 403.5317\n",
      "Epoch 277/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 653.6212\n",
      "Epoch 278/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 573.6837\n",
      "Epoch 279/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 490.6014\n",
      "Epoch 280/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 507.1903\n",
      "Epoch 281/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 620.6968\n",
      "Epoch 282/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 459.8192\n",
      "Epoch 283/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 412.0697\n",
      "Epoch 284/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 453.1612\n",
      "Epoch 285/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 488.7357\n",
      "Epoch 286/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 539.8013\n",
      "Epoch 287/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 504.6675\n",
      "Epoch 288/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 475.6625\n",
      "Epoch 289/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 421.6532\n",
      "Epoch 290/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 522.9534\n",
      "Epoch 291/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 467.5152\n",
      "Epoch 292/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 452.3990\n",
      "Epoch 293/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 604.8269\n",
      "Epoch 294/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 516.2417: 0s - loss: 532.75\n",
      "Epoch 295/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 601.6613\n",
      "Epoch 296/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 433.9819\n",
      "Epoch 297/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 507.1739\n",
      "Epoch 298/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 500.0485: 0s - loss: 408.758\n",
      "Epoch 299/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 428.3325\n",
      "Epoch 300/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 471.0016\n",
      "Epoch 301/1000\n",
      "2602/2602 [==============================] - 0s 45us/step - loss: 540.1374\n",
      "Epoch 302/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 598.6777\n",
      "Epoch 303/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 618.9716\n",
      "Epoch 304/1000\n",
      "2602/2602 [==============================] - 0s 47us/step - loss: 491.6288\n",
      "Epoch 305/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 510.3436\n",
      "Epoch 306/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 502.3345\n",
      "Epoch 307/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 481.7659\n",
      "Epoch 308/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 531.4533\n",
      "Epoch 309/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 464.5260\n",
      "Epoch 310/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 617.2762\n",
      "Epoch 311/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 519.9163\n",
      "Epoch 312/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 609.8532\n",
      "Epoch 313/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 530.4692\n",
      "Epoch 314/1000\n",
      "2602/2602 [==============================] - 0s 38us/step - loss: 540.8567\n",
      "Epoch 315/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 483.6415\n",
      "Epoch 316/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 434.0432\n",
      "Epoch 317/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 527.2687\n",
      "Epoch 318/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 519.2283\n",
      "Epoch 319/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 539.0179\n",
      "Epoch 320/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 447.0460\n",
      "Epoch 321/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 476.4714\n",
      "Epoch 322/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 536.8480\n",
      "Epoch 323/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 546.6769\n",
      "Epoch 324/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 444.2260\n",
      "Epoch 325/1000\n",
      "2602/2602 [==============================] - 0s 40us/step - loss: 491.5395\n",
      "Epoch 326/1000\n",
      "2602/2602 [==============================] - 0s 39us/step - loss: 491.3361\n",
      "Epoch 327/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 473.7346\n",
      "Epoch 328/1000\n",
      "2602/2602 [==============================] - 0s 43us/step - loss: 557.3159\n",
      "Epoch 329/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 543.7401\n",
      "Epoch 330/1000\n",
      "2602/2602 [==============================] - 0s 42us/step - loss: 499.7942\n",
      "Epoch 331/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 600.3372\n",
      "Epoch 332/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 508.4973\n",
      "Epoch 333/1000\n",
      "2602/2602 [==============================] - 0s 46us/step - loss: 560.9643\n",
      "Epoch 334/1000\n",
      "2602/2602 [==============================] - 0s 44us/step - loss: 453.3556\n",
      "Epoch 335/1000\n",
      "2602/2602 [==============================] - 0s 41us/step - loss: 485.0340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19441539eb8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add more dense layers\n",
    "NAME = \"{}-layers\".format(3)\n",
    "checkpointfunc2 = keras.callbacks.ModelCheckpoint('Keras Models/{}.model'.format(NAME), monitor='loss', verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False, mode='min', period=1)\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4, input_dim=14, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=1000, verbose=1, callbacks=[checkpointfunc2, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.7381788357430003 MSE:  232.98979591836735\n"
     ]
    }
   ],
   "source": [
    "y_test_pred2 = model.predict(np.array(X_test))\n",
    "y_test_pred2 = list(y_test_pred2)\n",
    "for i in range(len(y_test_pred2)):\n",
    "    if y_test_pred[i] < 0:\n",
    "        y_test_pred[i] = 0\n",
    "y_test_pred2 = np.round(y_test_pred2).astype(\"int\")\n",
    "R2 = explained_variance_score(y_test, y_test_pred2)\n",
    "MSE = mean_squared_error(y_test, y_test_pred2)\n",
    "    \n",
    "print(\"R2: \", R2, \"MSE: \", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding more dense layers (adjusting the hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Layers 4 finish R2:  0.7447204020878175 MSE:  225.2091836734694\n",
      "Dense Layers 5 finish R2:  0.8249915824510976 MSE:  154.4795918367347\n",
      "Dense Layers 6 finish R2:  0.8391713939245583 MSE:  142.1377551020408\n",
      "Dense Layers 7 finish R2:  0.0 MSE:  896.2602040816327\n",
      "Dense Layers 8 finish R2:  0.0 MSE:  889.7602040816327\n",
      "Dense Layers 9 finish R2:  0.0 MSE:  896.2602040816327\n",
      "Dense Layers 10 finish R2:  0.0 MSE:  885.2602040816327\n"
     ]
    }
   ],
   "source": [
    "dense_layers = list(range(4, 11))\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    NAME = \"{}-layers\".format(dense_layer)\n",
    "    # always save the best model\n",
    "    checkpointfunc = keras.callbacks.ModelCheckpoint('Keras Models/{}.model'.format(NAME), monitor='loss', verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(4, input_dim=14, activation='relu'))\n",
    "    for i in range(dense_layer):\n",
    "        model.add(keras.layers.Dense(4, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(X_train, y_train, epochs=1000, verbose=0, callbacks=[checkpointfunc, earlystopping])\n",
    "    \n",
    "    y_test_pred = model.predict(np.array(X_test))\n",
    "    y_test_pred = list(y_test_pred)\n",
    "    for i in range(len(y_test_pred)):\n",
    "        if y_test_pred[i] < 0:\n",
    "            y_test_pred[i] = 0\n",
    "    y_test_pred = np.round(y_test_pred).astype(\"int\")\n",
    "    R2 = explained_variance_score(y_test, y_test_pred)\n",
    "    MSE = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"Dense Layers\", dense_layer, \"finish\", \"R2: \", R2, \"MSE: \", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 dense layers has the highest R2 and the lowest MSE, hence the best model;\n",
    "### apply the model with 6 dense layers to predict the Summer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load a model\n",
    "NAME = \"{}-layers\".format(6)\n",
    "model = keras.models.load_model('Keras Models/{}.model'.format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred3 = model.predict(np.array(X_test))\n",
    "y_test_pred3 = list(y_test_pred3)\n",
    "for i in range(len(y_test_pred3)):\n",
    "    if y_test_pred3[i] < 0:\n",
    "        y_test_pred3[i] = 0\n",
    "y_test_pred3 = np.round(y_test_pred3).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_df = pd.DataFrame(y_test_pred3)\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "noc_df = pd.DataFrame(NOC_neat)\n",
    "result_summer = pd.concat([noc_df, y_test_df, y_test_pred_df], axis = 1)\n",
    "result_summer.columns = [\"Country\", \"True Value\", \"Predicted Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the csv file\n",
    "result_summer.to_csv(\"Results/Summer - Keras CNN Model.csv\", index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
